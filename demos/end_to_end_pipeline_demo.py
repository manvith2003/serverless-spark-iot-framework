#!/usr/bin/env python3
"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    COMPLETE END-TO-END PIPELINE DEMONSTRATION
                    IoT â†’ MQTT â†’ Kafka â†’ Spark â†’ RL â†’ YARN
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

This script simulates the COMPLETE data flow through all components:
1. IoT Sensors generating data
2. MQTT Broker receiving messages
3. Kafka topics storing streams
4. Spark processing data
5. Phase-1 RL making scaling decisions
6. YARN allocating resources
7. Phase-2 RL adjusting weights
8. Continuous learning loop

Run this to see the entire system working together!
"""

import time
import random
import numpy as np
from datetime import datetime
from dataclasses import dataclass
from typing import Dict, List, Tuple
import sys


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PRINT UTILITIES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class Colors:
    HEADER = '\033[95m'
    BLUE = '\033[94m'
    CYAN = '\033[96m'
    GREEN = '\033[92m'
    YELLOW = '\033[93m'
    RED = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'

def print_header(text: str):
    print(f"\n{Colors.BOLD}{Colors.HEADER}{'â•' * 80}{Colors.ENDC}")
    print(f"{Colors.BOLD}{Colors.HEADER}  {text}{Colors.ENDC}")
    print(f"{Colors.BOLD}{Colors.HEADER}{'â•' * 80}{Colors.ENDC}")

def print_section(text: str):
    print(f"\n{Colors.BOLD}{Colors.CYAN}â”Œ{'â”€' * 70}â”{Colors.ENDC}")
    print(f"{Colors.BOLD}{Colors.CYAN}â”‚  {text:<68}â”‚{Colors.ENDC}")
    print(f"{Colors.BOLD}{Colors.CYAN}â””{'â”€' * 70}â”˜{Colors.ENDC}")

def print_step(icon: str, text: str, color=Colors.GREEN):
    print(f"  {color}{icon}{Colors.ENDC}  {text}")

def print_metric(name: str, value: str, color=Colors.BLUE):
    print(f"      {color}â”œâ”€{Colors.ENDC} {name}: {Colors.BOLD}{value}{Colors.ENDC}")

def print_reward(text: str, value: float):
    color = Colors.GREEN if value > 0 else Colors.RED
    sign = "+" if value > 0 else ""
    print(f"      {color}â”‚  â†’ {text}: {sign}{value:.4f}{Colors.ENDC}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SIMULATED COMPONENTS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@dataclass
class IoTReading:
    sensor_id: str
    sensor_type: str
    value: float
    timestamp: str

@dataclass
class SparkMetrics:
    workload_rate: float
    latency_ms: float
    cpu_util: float
    cost: float
    throughput: float


class EndToEndPipelineDemo:
    """Complete pipeline demonstration"""
    
    def __init__(self):
        # System state
        self.current_executors = 2
        self.alpha = 0.33
        self.beta = 0.33
        self.gamma = 0.34
        
        # Business constraints
        self.sla_target = 200.0
        self.cost_budget = 10.0
        
        # Counters
        self.total_messages = 0
        self.sla_violations = 0
        self.episode_rewards = []
        
    def generate_iot_reading(self) -> IoTReading:
        """Simulate IoT sensor generating data"""
        sensor_types = ["temperature", "humidity", "air_quality", "traffic"]
        sensor_type = random.choice(sensor_types)
        
        values = {
            "temperature": random.uniform(20, 45),
            "humidity": random.uniform(30, 80),
            "air_quality": random.randint(50, 300),
            "traffic": random.randint(10, 500)
        }
        
        return IoTReading(
            sensor_id=f"sensor_{random.randint(1, 100):03d}",
            sensor_type=sensor_type,
            value=values[sensor_type],
            timestamp=datetime.now().isoformat()
        )
    
    def simulate_mqtt_publish(self, reading: IoTReading) -> bool:
        """Simulate MQTT broker receiving message"""
        return True
    
    def simulate_kafka_produce(self, reading: IoTReading) -> Dict:
        """Simulate Kafka producer"""
        return {
            "topic": "iot-raw-data",
            "partition": random.randint(0, 7),
            "offset": self.total_messages
        }
    
    def simulate_spark_process(self, batch_size: int) -> SparkMetrics:
        """Simulate Spark processing batch"""
        # Workload varies with batch size
        workload = batch_size * 10 + random.uniform(-20, 20)
        
        # Latency depends on workload vs executors
        base_latency = 200 * (workload / (self.current_executors * 50))
        latency = max(30, base_latency + random.uniform(-30, 30))
        
        # Cost is executor-based
        cost = self.current_executors * 0.5
        
        return SparkMetrics(
            workload_rate=workload,
            latency_ms=latency,
            cpu_util=min(100, 30 + workload / self.current_executors * 0.5),
            cost=cost,
            throughput=workload * 0.9
        )
    
    def phase1_decide(self, metrics: SparkMetrics) -> Tuple[str, int, float]:
        """Phase-1 RL makes scaling decision"""
        
        # Compute Phase-1 reward
        norm_cost = metrics.cost / 10.0
        norm_latency = metrics.latency_ms / 1000.0
        norm_throughput = metrics.throughput / 200.0
        
        reward = (
            -self.alpha * norm_cost +
            -self.beta * norm_latency +
            self.gamma * norm_throughput
        )
        
        # Simple policy based on latency and cost
        if metrics.latency_ms > self.sla_target * 1.2:
            action = "SCALE_UP"
            new_executors = min(20, self.current_executors + 2)
        elif metrics.latency_ms < self.sla_target * 0.5 and metrics.cost > self.cost_budget * 0.8:
            action = "SCALE_DOWN"
            new_executors = max(1, self.current_executors - 1)
        else:
            action = "MAINTAIN"
            new_executors = self.current_executors
        
        return action, new_executors, reward
    
    def yarn_allocate(self, target: int) -> Dict:
        """Simulate YARN allocating containers"""
        old_count = self.current_executors
        self.current_executors = target
        
        return {
            "old_count": old_count,
            "new_count": target,
            "containers_added": max(0, target - old_count),
            "containers_removed": max(0, old_count - target),
            "status": "SUCCESS"
        }
    
    def phase2_decide(self, episode_metrics: Dict) -> Tuple[float, float, float, float]:
        """Phase-2 RL adjusts weights"""
        
        avg_latency = episode_metrics["avg_latency"]
        total_cost = episode_metrics["total_cost"]
        violations = episode_metrics["violations"]
        
        # Compute Phase-2 reward
        if avg_latency <= self.sla_target:
            latency_reward = 1.0
        else:
            latency_reward = -1.0 * (avg_latency / self.sla_target - 1)
        
        if total_cost <= self.cost_budget:
            cost_reward = 1.0 - (total_cost / self.cost_budget) * 0.5
        else:
            cost_reward = -1.0 * (total_cost / self.cost_budget - 1)
        
        violation_penalty = -0.5 * violations
        
        meta_reward = latency_reward + cost_reward + violation_penalty
        
        # Adjust weights based on what's hurting
        if avg_latency > self.sla_target * 1.1:
            # Increase latency priority
            self.beta = min(0.7, self.beta + 0.05)
        elif total_cost > self.cost_budget:
            # Increase cost priority
            self.alpha = min(0.7, self.alpha + 0.05)
        
        # Normalize
        total = self.alpha + self.beta + self.gamma
        self.alpha /= total
        self.beta /= total
        self.gamma /= total
        
        return self.alpha, self.beta, self.gamma, meta_reward
    
    def run_demo(self, num_episodes: int = 3, steps_per_episode: int = 5):
        """Run the complete end-to-end demo"""
        
        print_header("SERVERLESS SPARK IoT FRAMEWORK - COMPLETE PIPELINE DEMO")
        
        print(f"""
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                                                          â”‚
    â”‚   IoT Sensors â†’ MQTT â†’ Kafka â†’ Spark â†’ Phase-1 RL â†’ YARN                â”‚
    â”‚                                    â†‘                                     â”‚
    â”‚                              Phase-2 RL (Meta)                           â”‚
    â”‚                                                                          â”‚
    â”‚   This demo shows the COMPLETE data flow through all components!         â”‚
    â”‚                                                                          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        """)
        
        time.sleep(1)
        
        for episode in range(num_episodes):
            print_header(f"EPISODE {episode + 1} / {num_episodes}")
            print(f"\n  {Colors.YELLOW}Current Weights:{Colors.ENDC} "
                  f"Î±(cost)={self.alpha:.3f}, Î²(latency)={self.beta:.3f}, Î³(throughput)={self.gamma:.3f}")
            print(f"  {Colors.YELLOW}Current Executors:{Colors.ENDC} {self.current_executors}\n")
            
            episode_latencies = []
            episode_costs = []
            episode_violations = 0
            episode_rewards_sum = 0
            
            for step in range(steps_per_episode):
                print_section(f"STEP {step + 1} / {steps_per_episode}")
                time.sleep(0.3)
                
                # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                # 1. IoT SENSORS
                # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                print_step("ğŸŒ¡ï¸", "IoT SENSOR generating data...")
                readings = [self.generate_iot_reading() for _ in range(random.randint(5, 15))]
                print_metric("Sensors active", f"{len(readings)}")
                print_metric("Sample reading", f"{readings[0].sensor_type}: {readings[0].value:.2f}")
                time.sleep(0.2)
                
                # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                # 2. MQTT BROKER
                # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                print_step("ğŸ“¡", "MQTT BROKER receiving messages...")
                for r in readings:
                    self.simulate_mqtt_publish(r)
                print_metric("Messages received", f"{len(readings)}")
                print_metric("Topic", "iot/+/data")
                time.sleep(0.2)
                
                # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                # 3. KAFKA
                # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                print_step("ğŸ“¨", "KAFKA storing to topics...")
                kafka_results = [self.simulate_kafka_produce(r) for r in readings]
                self.total_messages += len(readings)
                print_metric("Topic", "iot-raw-data")
                print_metric("Partitions used", f"{len(set(k['partition'] for k in kafka_results))}")
                print_metric("Total offset", f"{self.total_messages}")
                time.sleep(0.2)
                
                # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                # 4. SPARK PROCESSING
                # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                print_step("âš¡", "SPARK STREAMING processing batch...")
                spark_metrics = self.simulate_spark_process(len(readings))
                print_metric("Workload", f"{spark_metrics.workload_rate:.1f} msg/s")
                print_metric("Latency", f"{spark_metrics.latency_ms:.1f} ms " + 
                            (f"{Colors.GREEN}âœ“{Colors.ENDC}" if spark_metrics.latency_ms <= self.sla_target 
                             else f"{Colors.RED}âœ— (SLA: {self.sla_target}ms){Colors.ENDC}"))
                print_metric("CPU Utilization", f"{spark_metrics.cpu_util:.1f}%")
                print_metric("Cost", f"${spark_metrics.cost:.2f}/hr")
                
                if spark_metrics.latency_ms > self.sla_target:
                    episode_violations += 1
                    self.sla_violations += 1
                
                episode_latencies.append(spark_metrics.latency_ms)
                episode_costs.append(spark_metrics.cost)
                time.sleep(0.2)
                
                # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                # 5. PHASE-1 RL DECISION
                # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                print_step("ğŸ¤–", "PHASE-1 RL (PPO) making decision...")
                action, new_executors, reward = self.phase1_decide(spark_metrics)
                
                print_metric("State", f"[workload={spark_metrics.workload_rate:.0f}, "
                            f"lat={spark_metrics.latency_ms:.0f}, cpu={spark_metrics.cpu_util:.0f}]")
                print_metric("Action", f"{action}")
                
                # Show reward breakdown
                print(f"\n      {Colors.CYAN}â”‚  REWARD CALCULATION:{Colors.ENDC}")
                print_reward(f"Cost penalty     (-Î± Ã— {spark_metrics.cost/10:.3f})", 
                            -self.alpha * spark_metrics.cost / 10)
                print_reward(f"Latency penalty  (-Î² Ã— {spark_metrics.latency_ms/1000:.3f})", 
                            -self.beta * spark_metrics.latency_ms / 1000)
                print_reward(f"Throughput bonus (+Î³ Ã— {spark_metrics.throughput/200:.3f})", 
                            self.gamma * spark_metrics.throughput / 200)
                print(f"      {Colors.BOLD}â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•{Colors.ENDC}")
                color = Colors.GREEN if reward > 0 else Colors.RED
                print(f"      {color}â”‚  TOTAL REWARD: {'+' if reward > 0 else ''}{reward:.4f}{Colors.ENDC}")
                
                episode_rewards_sum += reward
                self.episode_rewards.append(reward)
                time.sleep(0.2)
                
                # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                # 6. YARN ALLOCATION
                # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                if action != "MAINTAIN":
                    print_step("ğŸ”§", "YARN RESOURCE MANAGER allocating...")
                    yarn_result = self.yarn_allocate(new_executors)
                    print_metric("Previous executors", f"{yarn_result['old_count']}")
                    print_metric("New executors", f"{yarn_result['new_count']}")
                    if yarn_result['containers_added'] > 0:
                        print_metric("Containers ADDED", f"+{yarn_result['containers_added']} ğŸš€")
                    if yarn_result['containers_removed'] > 0:
                        print_metric("Containers REMOVED", f"-{yarn_result['containers_removed']} ğŸ›‘")
                    print_metric("Status", f"{Colors.GREEN}{yarn_result['status']}{Colors.ENDC}")
                else:
                    print_step("â¸ï¸", f"YARN: No change needed (keeping {self.current_executors} executors)")
                
                print()
                time.sleep(0.3)
            
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # 7. PHASE-2 META-RL UPDATE
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            print_section(f"PHASE-2 META-RL UPDATE (End of Episode {episode + 1})")
            
            episode_data = {
                "avg_latency": np.mean(episode_latencies),
                "total_cost": np.sum(episode_costs),
                "violations": episode_violations
            }
            
            old_alpha, old_beta, old_gamma = self.alpha, self.beta, self.gamma
            new_alpha, new_beta, new_gamma, meta_reward = self.phase2_decide(episode_data)
            
            print_step("ğŸ§ ", "PHASE-2 RL (PPO) observing episode performance...")
            print_metric("Avg Latency", f"{episode_data['avg_latency']:.1f} ms")
            print_metric("Total Cost", f"${episode_data['total_cost']:.2f}")
            print_metric("SLA Violations", f"{episode_violations}")
            print_metric("Episode Reward Sum", f"{episode_rewards_sum:.4f}")
            
            print(f"\n      {Colors.CYAN}â”‚  META-REWARD CALCULATION:{Colors.ENDC}")
            if episode_data['avg_latency'] <= self.sla_target:
                print_reward("SLA Compliance", 1.0)
            else:
                print_reward("SLA Penalty", -1.0 * (episode_data['avg_latency'] / self.sla_target - 1))
            
            if episode_data['total_cost'] <= self.cost_budget:
                print_reward("Budget Compliance", 1.0 - (episode_data['total_cost'] / self.cost_budget) * 0.5)
            else:
                print_reward("Budget Penalty", -1.0 * (episode_data['total_cost'] / self.cost_budget - 1))
            
            print_reward("Violation Penalty", -0.5 * episode_violations)
            print(f"      {Colors.BOLD}â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•{Colors.ENDC}")
            color = Colors.GREEN if meta_reward > 0 else Colors.RED
            print(f"      {color}â”‚  META-REWARD: {'+' if meta_reward > 0 else ''}{meta_reward:.4f}{Colors.ENDC}")
            
            print(f"\n      {Colors.YELLOW}â”‚  WEIGHT UPDATE:{Colors.ENDC}")
            print(f"      â”‚  Old: Î±={old_alpha:.3f}, Î²={old_beta:.3f}, Î³={old_gamma:.3f}")
            print(f"      â”‚  New: Î±={new_alpha:.3f}, Î²={new_beta:.3f}, Î³={new_gamma:.3f}")
            
            if new_beta > new_alpha and new_beta > new_gamma:
                priority = "LATENCY"
            elif new_alpha > new_beta and new_alpha > new_gamma:
                priority = "COST"
            else:
                priority = "THROUGHPUT"
            print(f"      {Colors.BOLD}â”‚  â†’ Priority: {priority}{Colors.ENDC}")
            
            time.sleep(0.5)
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # FINAL SUMMARY
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        print_header("DEMO COMPLETE - SUMMARY")
        
        print(f"""
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                          PIPELINE STATISTICS                             â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚  Total Messages Processed:  {self.total_messages:>8}                                   â”‚
    â”‚  Total SLA Violations:      {self.sla_violations:>8}                                   â”‚
    â”‚  Final Executors:           {self.current_executors:>8}                                   â”‚
    â”‚                                                                          â”‚
    â”‚  Final Weights:                                                          â”‚
    â”‚    Î± (cost):       {self.alpha:>8.3f}                                          â”‚
    â”‚    Î² (latency):    {self.beta:>8.3f}                                          â”‚
    â”‚    Î³ (throughput): {self.gamma:>8.3f}                                          â”‚
    â”‚                                                                          â”‚
    â”‚  Average Step Reward:       {np.mean(self.episode_rewards):>+8.4f}                             â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        """)
        
        print(f"\n  {Colors.GREEN}âœ… Both Phase-1 and Phase-2 RL are learning continuously!{Colors.ENDC}")
        print(f"  {Colors.GREEN}âœ… YARN dynamically allocates/deallocates executors!{Colors.ENDC}")
        print(f"  {Colors.GREEN}âœ… Rewards and penalties drive optimization!{Colors.ENDC}\n")


if __name__ == "__main__":
    demo = EndToEndPipelineDemo()
    demo.run_demo(num_episodes=3, steps_per_episode=3)
