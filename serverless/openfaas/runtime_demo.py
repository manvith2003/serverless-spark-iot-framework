"""
Runtime Scenario Demonstration
Shows exactly what happens at runtime for both cases
"""

import json
import time
import sys
import os

# Add path for the handler
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)) + '/rl-scaling-function')
from handler import RLScalingHandler, handle


def display_workflow(steps: list, title: str):
    """Display workflow steps with arrows"""
    print(f"\n{'='*60}")
    print(f"ðŸŽ¬ {title}")
    print('='*60)
    for i, step in enumerate(steps):
        print(f"   {step}")
        if i < len(steps) - 1:
            print("   â†“")
    print()


def simulate_case_1_low_traffic():
    """
    Case 1ï¸âƒ£: Low IoT traffic (night time)
    Result: Scale down to ZERO executors
    """
    
    display_workflow([
        "IoT traffic low (5 msg/s)",
        "Spark Driver sees low load",
        "Driver calls OpenFaaS (RL)",
        "RL says: scale_down = ALL",
        "Driver calls: killExecutor(...)",
        "YARN kills containers",
        "ZERO Spark executor containers running"
    ], "Case 1ï¸âƒ£: Low IoT Traffic (Night Time)")
    
    # Simulate night time metrics
    metrics = {
        "workload_rate": 5.0,      # Very low - night time
        "cpu_util": 8.0,           # Almost idle
        "mem_util": 15.0,
        "current_executors": 4,    # Currently running 4 executors
        "latency_ms": 50,          # Low latency (no backlog)
        "kafka_lag": 0,            # No lag
        "time_of_day": "night"
    }
    
    print(f"ðŸ“Š Input Metrics:")
    print(f"   â€¢ Workload: {metrics['workload_rate']} msg/s (very low)")
    print(f"   â€¢ CPU: {metrics['cpu_util']}% (idle)")
    print(f"   â€¢ Current Executors: {metrics['current_executors']}")
    print(f"   â€¢ Kafka Lag: {metrics['kafka_lag']} messages")
    
    # Call OpenFaaS RL function
    print(f"\nðŸ“ž Calling OpenFaaS RL Function...")
    result = handle(json.dumps(metrics))
    decision = json.loads(result)
    
    print(f"\nðŸ¤– RL Decision:")
    print(f"   â€¢ Action: {decision['action'].upper()}")
    print(f"   â€¢ Target Executors: {decision['target_executors']}")
    print(f"   â€¢ Executors to Kill: {decision['executors_to_kill']}")
    print(f"   â€¢ Reason: {decision['reason']}")
    print(f"   â€¢ Confidence: {decision['confidence']:.0%}")
    
    # Simulate executor killing
    print(f"\nâš¡ Executing...")
    for i in range(metrics['current_executors']):
        time.sleep(0.2)
        print(f"   âœ‚ï¸  killExecutor(executor-{i}) â†’ YARN removing container...")
    
    print(f"\nâœ… RESULT:")
    print(f"   â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print(f"   â•‘  ðŸŽ¯ ZERO Spark executor containers     â•‘")
    print(f"   â•‘  ðŸ’° No wasted resources                â•‘")
    print(f"   â•‘  âš¡ Almost serverless behavior         â•‘")
    print(f"   â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    
    return decision


def simulate_case_2_traffic_spike():
    """
    Case 2ï¸âƒ£: Temperature spike (day time)  
    Result: Scale up to handle spike
    """
    
    display_workflow([
        "IoT traffic spike (500+ msg/s)",
        "Kafka lag increases (1000+ messages)",
        "Spark Driver detects backlog",
        "Driver calls OpenFaaS (RL)",
        "RL says: scale_up = 6",
        "Driver calls: requestExecutors(6)",
        "YARN allocates 6 containers",
        "Spark executors start processing"
    ], "Case 2ï¸âƒ£: Temperature Spike (Day Time)")
    
    # Simulate spike metrics
    metrics = {
        "workload_rate": 500.0,    # High - spike!
        "cpu_util": 95.0,          # Overloaded
        "mem_util": 80.0,
        "current_executors": 2,    # Only 2 running (was scaled down)
        "latency_ms": 3000,        # High latency!
        "kafka_lag": 5000,         # Backlog building up!
        "time_of_day": "day"
    }
    
    print(f"ðŸ“Š Input Metrics:")
    print(f"   â€¢ Workload: {metrics['workload_rate']} msg/s (SPIKE! ðŸ”¥)")
    print(f"   â€¢ CPU: {metrics['cpu_util']}% (overloaded)")
    print(f"   â€¢ Current Executors: {metrics['current_executors']}")
    print(f"   â€¢ Kafka Lag: {metrics['kafka_lag']} messages (BACKLOG!)")
    print(f"   â€¢ Latency: {metrics['latency_ms']}ms (HIGH!)")
    
    # Call OpenFaaS RL function
    print(f"\nðŸ“ž Calling OpenFaaS RL Function...")
    result = handle(json.dumps(metrics))
    decision = json.loads(result)
    
    print(f"\nðŸ¤– RL Decision:")
    print(f"   â€¢ Action: {decision['action'].upper()}")
    print(f"   â€¢ Target Executors: {decision['target_executors']}")
    print(f"   â€¢ Executors to Add: {decision['executors_to_add']}")
    print(f"   â€¢ Reason: {decision['reason']}")
    print(f"   â€¢ Confidence: {decision['confidence']:.0%}")
    
    # Simulate executor allocation
    print(f"\nâš¡ Executing...")
    for i in range(decision['executors_to_add']):
        time.sleep(0.2)
        print(f"   âž• requestExecutors() â†’ YARN allocating container executor-{metrics['current_executors'] + i}...")
    
    print(f"\nâœ… RESULT:")
    print(f"   â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print(f"   â•‘  ðŸŽ¯ {decision['target_executors']} Spark executors running       â•‘")
    print(f"   â•‘  ðŸ“ˆ Processing spike traffic           â•‘")
    print(f"   â•‘  â±ï¸  Containers exist only during spike â•‘")
    print(f"   â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    
    return decision


def simulate_case_3_edge_prediction():
    """
    Case 3ï¸âƒ£: Edge predicts burst BEFORE it happens
    Result: Pre-scale to handle upcoming spike
    """
    
    display_workflow([
        "Edge node predicts burst (ML model)",
        "Sends burst_signal=1.0 to Kafka metadata topic",
        "Spark Driver reads metadata",
        "Driver calls OpenFaaS (RL) with edge signal",
        "RL says: PRE-SCALE to 10 executors",
        "Driver calls: requestExecutors(6)",
        "YARN allocates containers BEFORE spike",
        "System ready when spike arrives!"
    ], "Case 3ï¸âƒ£: Edge Burst Prediction (Proactive)")
    
    # Current load is normal, but edge predicts burst
    metrics = {
        "workload_rate": 100.0,           # Normal now
        "cpu_util": 50.0,                 # Normal
        "mem_util": 45.0,
        "current_executors": 4,
        "latency_ms": 300,
        "edge_burst_signal": 1.0,          # ðŸš¨ BURST PREDICTED!
        "edge_predicted_workload": 600.0   # Expects 600 msg/s soon
    }
    
    print(f"ðŸ“Š Input Metrics:")
    print(f"   â€¢ Current Workload: {metrics['workload_rate']} msg/s (normal)")
    print(f"   â€¢ CPU: {metrics['cpu_util']}% (normal)")
    print(f"   â€¢ Current Executors: {metrics['current_executors']}")
    print(f"   â€¢ ðŸš¨ EDGE BURST SIGNAL: {metrics['edge_burst_signal']}")
    print(f"   â€¢ Predicted Workload: {metrics['edge_predicted_workload']} msg/s")
    
    # Call OpenFaaS RL function
    print(f"\nðŸ“ž Calling OpenFaaS RL Function...")
    result = handle(json.dumps(metrics))
    decision = json.loads(result)
    
    print(f"\nðŸ¤– RL Decision:")
    print(f"   â€¢ Action: {decision['action'].upper()}")
    print(f"   â€¢ Target Executors: {decision['target_executors']}")
    print(f"   â€¢ Executors to Add: {decision['executors_to_add']}")
    print(f"   â€¢ Reason: {decision['reason']}")
    print(f"   â€¢ Confidence: {decision['confidence']:.0%}")
    
    print(f"\nâœ… RESULT:")
    print(f"   â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print(f"   â•‘  ðŸŽ¯ PRE-SCALED to {decision['target_executors']} executors          â•‘")
    print(f"   â•‘  âš¡ Ready BEFORE spike arrives             â•‘")
    print(f"   â•‘  ðŸ§  Edge intelligence enables proactive    â•‘")
    print(f"   â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    
    return decision


def main():
    print("\n" + "="*60)
    print("ðŸŽ¬ SERVERLESS SPARK RUNTIME DEMONSTRATION")
    print("="*60)
    print("""
This demo shows exactly what happens at runtime:
â€¢ Case 1: Night time low traffic â†’ Scale to ZERO
â€¢ Case 2: Day time spike â†’ Scale UP to handle load  
â€¢ Case 3: Edge prediction â†’ PRE-SCALE before spike
    """)
    
    input("Press Enter to start Case 1 (Low Traffic)...")
    case1 = simulate_case_1_low_traffic()
    
    input("\nPress Enter to start Case 2 (Traffic Spike)...")
    case2 = simulate_case_2_traffic_spike()
    
    input("\nPress Enter to start Case 3 (Edge Prediction)...")
    case3 = simulate_case_3_edge_prediction()
    
    # Summary
    print("\n" + "="*60)
    print("ðŸ“Š SUMMARY: All Runtime Scenarios")
    print("="*60)
    print("""
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Scenario        â”‚ Executors    â”‚ Behavior            â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ Low Traffic     â”‚ 4 â†’ 0        â”‚ ZERO containers âœ…  â”‚
    â”‚ Traffic Spike   â”‚ 2 â†’ 8        â”‚ Scale up for spike  â”‚
    â”‚ Edge Prediction â”‚ 4 â†’ 10       â”‚ Pre-scale proactive â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    âœ… Serverless behavior achieved!
    âœ… Resources exist only when needed!
    âœ… Edge intelligence enables proactive scaling!
    """)


if __name__ == "__main__":
    main()
