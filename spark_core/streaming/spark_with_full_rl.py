#!/usr/bin/env python3
"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            SPARK STREAMING WITH FULLY TRAINED TWO-RL (NO RULE-BASED LOGIC!)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

This script uses TRAINED PPO neural networks for ALL decisions:
  - Phase-1 RL: PPO model decides SCALE_UP / SCALE_DOWN / MAINTAIN
  - Phase-2 RL: TD3 model decides weight updates (Î±, Î², Î³)

NO HARDCODED THRESHOLDS! Everything is learned by the RL agents.

Prerequisites:
  - Kafka running on localhost:9092
  - Redis running on localhost:6379
  - Trained models in data/models/

Run:
  python3 spark_core/streaming/spark_with_full_rl.py
"""

from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.types import *
import numpy as np
import builtins
import time
import json
import os
import sys
from datetime import datetime
from typing import Dict, Tuple, Optional
from dataclasses import dataclass

# Try to import RL libraries
try:
    from stable_baselines3 import PPO, TD3
    RL_AVAILABLE = True
except ImportError:
    RL_AVAILABLE = False
    print("âš ï¸  stable-baselines3 not installed. Run: pip install stable-baselines3")

# Try to import Redis
try:
    import redis
    REDIS_AVAILABLE = True
except ImportError:
    REDIS_AVAILABLE = False
    print("âš ï¸  redis not installed. RL state will not persist.")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PRINT UTILITIES  
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class Colors:
    HEADER = '\033[95m'
    BLUE = '\033[94m'
    CYAN = '\033[96m'
    GREEN = '\033[92m'
    YELLOW = '\033[93m'
    RED = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'
    MAGENTA = '\033[35m'


def print_header(text: str):
    print(f"\n{Colors.BOLD}{Colors.HEADER}{'â•' * 80}{Colors.ENDC}")
    print(f"{Colors.BOLD}{Colors.HEADER}  {text}{Colors.ENDC}")
    print(f"{Colors.BOLD}{Colors.HEADER}{'â•' * 80}{Colors.ENDC}")


def print_metric(name: str, value: str, indent: int = 6):
    spaces = " " * indent
    print(f"{spaces}{Colors.BLUE}â”œâ”€{Colors.ENDC} {name}: {Colors.BOLD}{value}{Colors.ENDC}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# FULLY RL-BASED CONTROLLER (NO RULES!)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class FullRLController:
    """
    Fully RL-Based Controller using trained PPO and TD3 models.
    
    NO HARDCODED THRESHOLDS! The neural networks make ALL decisions.
    
    Phase-1 (PPO): Scaling decisions based on 13-dim state
    Phase-2 (TD3): Weight updates based on episode performance
    """
    
    # Action mapping
    ACTION_NAMES = {0: "SCALE_DOWN", 1: "MAINTAIN", 2: "SCALE_UP"}
    
    def __init__(self,
                 phase1_model_path: str = "data/models/ppo_resource_allocator/best_model/best_model.zip",
                 phase2_model_path: str = "data/models/meta_controller/td3_meta_agent.zip",
                 sla_target_ms: float = 200.0,
                 cost_budget_per_hour: float = 10.0,
                 episode_length: int = 10,
                 min_executors: int = 1,
                 max_executors: int = 20,
                 redis_host: str = "localhost",
                 redis_port: int = 6379):
        
        # Business constraints
        self.sla_target = sla_target_ms
        self.cost_budget = cost_budget_per_hour
        self.episode_length = episode_length
        self.min_executors = min_executors
        self.max_executors = max_executors
        
        # RL State
        self.alpha = 0.33  # Cost weight
        self.beta = 0.33   # Latency weight
        self.gamma = 0.34  # Throughput weight
        self.current_executors = 2
        
        # Episode tracking
        self.batch_count = 0
        self.episode_count = 0
        self.episode_latencies = []
        self.episode_costs = []
        self.episode_violations = 0
        self.episode_rewards = []
        self.episode_throughputs = []
        
        # Global stats
        self.total_batches = 0
        self.total_messages = 0
        self.total_violations = 0
        
        # Load trained models
        self.phase1_model = None
        self.phase2_model = None
        self._load_models(phase1_model_path, phase2_model_path)
        
        # Redis connection
        self.redis_client = None
        if REDIS_AVAILABLE:
            try:
                self.redis_client = redis.Redis(host=redis_host, port=redis_port, db=0)
                self.redis_client.ping()
                self._load_state_from_redis()
                print(f"  {Colors.GREEN}âœ“{Colors.ENDC} Redis connected: {redis_host}:{redis_port}")
            except Exception as e:
                print(f"  {Colors.YELLOW}âš {Colors.ENDC} Redis not available: {e}")
                self.redis_client = None
    
    def _load_models(self, phase1_path: str, phase2_path: str):
        """Load trained RL models"""
        if not RL_AVAILABLE:
            print(f"  {Colors.RED}âœ—{Colors.ENDC} stable-baselines3 not available!")
            return
        
        # Get project root
        project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        
        # Load Phase-1 PPO model
        phase1_full_path = os.path.join(project_root, phase1_path)
        if os.path.exists(phase1_full_path):
            try:
                self.phase1_model = PPO.load(phase1_full_path)
                print(f"  {Colors.GREEN}âœ“{Colors.ENDC} Phase-1 PPO model loaded: {phase1_path}")
            except Exception as e:
                print(f"  {Colors.YELLOW}âš {Colors.ENDC} Phase-1 model load error: {e}")
        else:
            print(f"  {Colors.YELLOW}âš {Colors.ENDC} Phase-1 model not found: {phase1_full_path}")
        
        # Load Phase-2 TD3 model
        phase2_full_path = os.path.join(project_root, phase2_path)
        if os.path.exists(phase2_full_path):
            try:
                self.phase2_model = TD3.load(phase2_full_path)
                print(f"  {Colors.GREEN}âœ“{Colors.ENDC} Phase-2 TD3 model loaded: {phase2_path}")
            except Exception as e:
                print(f"  {Colors.YELLOW}âš {Colors.ENDC} Phase-2 model load error: {e}")
        else:
            print(f"  {Colors.YELLOW}âš {Colors.ENDC} Phase-2 model not found: {phase2_full_path}")
    
    def _load_state_from_redis(self):
        """Load previous RL state from Redis"""
        if not self.redis_client:
            return
        
        try:
            weights = self.redis_client.hgetall("rl:weights")
            if weights:
                self.alpha = float(weights.get(b"alpha", 0.33))
                self.beta = float(weights.get(b"beta", 0.33))
                self.gamma = float(weights.get(b"gamma", 0.34))
            
            executors = self.redis_client.get("rl:executors")
            if executors:
                self.current_executors = int(executors)
            
            print(f"  {Colors.GREEN}âœ“{Colors.ENDC} Loaded RL state from Redis")
        except Exception as e:
            print(f"  {Colors.YELLOW}âš {Colors.ENDC} Could not load Redis state: {e}")
    
    def _save_state_to_redis(self):
        """Save current RL state to Redis"""
        if not self.redis_client:
            return
        
        try:
            self.redis_client.hset("rl:weights", mapping={
                "alpha": str(self.alpha),
                "beta": str(self.beta),
                "gamma": str(self.gamma)
            })
            self.redis_client.set("rl:executors", self.current_executors)
            self.redis_client.set("rl:total_batches", self.total_batches)
            self.redis_client.set("rl:total_messages", self.total_messages)
        except Exception:
            pass
    
    def _build_phase1_state(self, metrics: Dict) -> np.ndarray:
        """
        Build 13-dimensional state vector for Phase-1 PPO model.
        
        State dimensions:
        [0] workload_rate      - Normalized workload (msg/s)
        [1] data_volume        - Normalized data volume
        [2] cpu_util           - CPU utilization %
        [3] memory_util        - Memory utilization %
        [4] current_latency    - Normalized latency
        [5] current_cost       - Normalized cost
        [6] alpha              - Cost weight
        [7] beta               - Latency weight
        [8] gamma              - Throughput weight
        [9] shuffle_size       - Normalized shuffle size
        [10] data_temperature  - Hot/cold data indicator
        [11] edge_workload     - Edge-predicted workload
        [12] burst_signal      - Burst indicator
        """
        state = np.array([
            metrics.get("workload", 100) / 1000.0,           # [0] workload normalized
            metrics.get("data_volume_mb", 10) / 500.0,       # [1] data volume normalized
            metrics.get("cpu_util", 50) / 100.0,             # [2] CPU %
            metrics.get("memory_util", 50) / 100.0,          # [3] Memory %
            metrics.get("latency_ms", 150) / 1000.0,         # [4] latency normalized
            (self.current_executors * 0.5) / 10.0,           # [5] cost normalized
            self.alpha,                                       # [6] Î± weight
            self.beta,                                        # [7] Î² weight
            self.gamma,                                       # [8] Î³ weight
            metrics.get("shuffle_mb", 5) / 100.0,            # [9] shuffle normalized
            metrics.get("data_temp", 0.5),                   # [10] data temperature
            metrics.get("edge_workload", 100) / 1000.0,      # [11] edge prediction
            metrics.get("burst_signal", 0.0)                 # [12] burst indicator
        ], dtype=np.float32)
        
        return state
    
    def _build_phase2_state(self) -> np.ndarray:
        """
        Build state vector for Phase-2 TD3 model.
        
        The TD3 model was trained with a 3-dimensional state:
        [0] avg_latency_normalized   - Episode average latency / SLA
        [1] budget_utilization       - Cost / Budget ratio  
        [2] sla_compliance_rate      - % of batches meeting SLA
        """
        avg_latency = np.mean(self.episode_latencies) if self.episode_latencies else 150
        avg_cost = np.mean(self.episode_costs) if self.episode_costs else 2
        sla_compliant = sum(1 for l in self.episode_latencies if l <= self.sla_target)
        sla_rate = sla_compliant / len(self.episode_latencies) if self.episode_latencies else 1.0
        
        state = np.array([
            avg_latency / self.sla_target,          # [0] latency ratio
            avg_cost / self.cost_budget,            # [1] budget utilization
            sla_rate                                # [2] SLA compliance
        ], dtype=np.float32)
        
        return state
    
    def phase1_decide(self, metrics: Dict) -> Tuple[str, int, float]:
        """
        Phase-1 RL: Use TRAINED PPO model to decide scaling action.
        
        NO HARDCODED RULES! The neural network decides everything.
        """
        # Build state vector
        state = self._build_phase1_state(metrics)
        
        # Get action from trained model
        if self.phase1_model is not None:
            action, _ = self.phase1_model.predict(state, deterministic=True)
            # Handle both scalar and array outputs
            if hasattr(action, '__len__'):
                action = int(action[0]) if len(action) > 0 else 1
            else:
                action = int(action)
            # Ensure action is valid (0, 1, or 2)
            action = builtins.max(0, builtins.min(2, action))
        else:
            # Fallback to random if model not loaded
            action = np.random.choice([0, 1, 2], p=[0.1, 0.8, 0.1])
        
        # Map action to executor change
        action_name = self.ACTION_NAMES[action]
        
        if action == 0:  # SCALE_DOWN
            new_executors = builtins.max(self.min_executors, self.current_executors - 1)
        elif action == 2:  # SCALE_UP
            new_executors = builtins.min(self.max_executors, self.current_executors + 2)
        else:  # MAINTAIN
            new_executors = self.current_executors
        
        # Calculate reward (for monitoring, not training)
        workload = metrics.get("workload", 100)
        latency = metrics.get("latency_ms", 150)
        cost = self.current_executors * 0.5
        throughput = metrics.get("throughput", workload * 0.9)
        
        norm_cost = cost / 10.0
        norm_latency = latency / 1000.0
        norm_throughput = throughput / 200.0
        
        reward = (
            -self.alpha * norm_cost +
            -self.beta * norm_latency +
            self.gamma * norm_throughput
        )
        
        # Update state
        old_executors = self.current_executors
        self.current_executors = new_executors
        
        # Track for episode
        self.episode_latencies.append(latency)
        self.episode_costs.append(cost)
        self.episode_rewards.append(reward)
        self.episode_throughputs.append(throughput)
        
        if latency > self.sla_target:
            self.episode_violations += 1
            self.total_violations += 1
        
        return action_name, new_executors, reward
    
    def phase2_update(self) -> Tuple[float, float, float, float]:
        """
        Phase-2 RL: Use TRAINED TD3 model to update weights.
        
        NO HARDCODED RULES! The neural network decides the new weights.
        """
        if len(self.episode_latencies) == 0:
            return self.alpha, self.beta, self.gamma, 0.0
        
        # Build state vector
        state = self._build_phase2_state()
        
        # Get new weights from trained model
        if self.phase2_model is not None:
            action, _ = self.phase2_model.predict(state, deterministic=True)
            # TD3 outputs continuous values, apply softmax to get valid weights
            raw_weights = action[:3] if len(action) >= 3 else [0.33, 0.33, 0.34]
            # Shift to positive and normalize
            shifted = np.exp(raw_weights - np.max(raw_weights))
            normalized = shifted / shifted.sum()
            
            self.alpha = float(normalized[0])
            self.beta = float(normalized[1])
            self.gamma = float(normalized[2])
        # Else: keep existing weights
        
        # Calculate meta-reward (for monitoring)
        avg_latency = np.mean(self.episode_latencies)
        total_cost = np.sum(self.episode_costs)
        violations = self.episode_violations
        
        if avg_latency <= self.sla_target:
            latency_reward = 1.0
        else:
            latency_reward = -1.0 * (avg_latency / self.sla_target - 1)
        
        if total_cost <= self.cost_budget:
            cost_reward = 1.0 - (total_cost / self.cost_budget) * 0.5
        else:
            cost_reward = -1.0 * (total_cost / self.cost_budget - 1)
        
        violation_penalty = -0.5 * violations
        meta_reward = latency_reward + cost_reward + violation_penalty
        
        # Store in Redis
        if self.redis_client:
            self.redis_client.lpush("rl:meta_rewards", f"{time.time()}:{meta_reward:.4f}")
            self.redis_client.lpush("rl:episodes", json.dumps({
                "episode": self.episode_count,
                "avg_latency": float(avg_latency),
                "total_cost": float(total_cost),
                "violations": violations,
                "meta_reward": float(meta_reward),
                "weights": {"alpha": self.alpha, "beta": self.beta, "gamma": self.gamma}
            }))
        
        # Reset episode tracking
        self.episode_latencies = []
        self.episode_costs = []
        self.episode_violations = 0
        self.episode_rewards = []
        self.episode_throughputs = []
        self.episode_count += 1
        
        return self.alpha, self.beta, self.gamma, meta_reward
    
    def process_batch(self, batch_metrics: Dict) -> Dict:
        """
        Process one batch with FULLY RL-BASED decisions.
        
        Both Phase-1 and Phase-2 use trained neural networks!
        """
        self.batch_count += 1
        self.total_batches += 1
        self.total_messages += batch_metrics.get("message_count", 0)
        
        # Phase-1: Neural network scaling decision
        action, new_executors, reward = self.phase1_decide(batch_metrics)
        
        result = {
            "batch": self.batch_count,
            "action": action,
            "executors": new_executors,
            "reward": reward,
            "weights": {"alpha": self.alpha, "beta": self.beta, "gamma": self.gamma},
            "model_used": "PPO" if self.phase1_model else "Fallback"
        }
        
        # Phase-2: Check if episode complete
        if self.batch_count >= self.episode_length:
            alpha, beta, gamma, meta_reward = self.phase2_update()
            result["episode_complete"] = True
            result["episode"] = self.episode_count
            result["meta_reward"] = meta_reward
            result["new_weights"] = {"alpha": alpha, "beta": beta, "gamma": gamma}
            result["meta_model_used"] = "TD3" if self.phase2_model else "Fallback"
            self.batch_count = 0
        else:
            result["episode_complete"] = False
        
        # Save state
        self._save_state_to_redis()
        
        return result


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SPARK STREAMING WITH FULL RL
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class SparkWithFullRL:
    """
    Spark Structured Streaming with FULLY TRAINED RL models.
    
    NO HARDCODED THRESHOLDS! Everything is learned by neural networks.
    """
    
    def __init__(self,
                 kafka_bootstrap: str = "localhost:9092",
                 kafka_topic: str = "iot-realtime-demo",
                 batch_interval: str = "10 seconds"):
        
        self.kafka_bootstrap = kafka_bootstrap
        self.kafka_topic = kafka_topic
        self.batch_interval = batch_interval
        
        # Initialize FULL RL controller
        self.rl_controller = FullRLController(
            sla_target_ms=200.0,
            cost_budget_per_hour=10.0,
            episode_length=5  # 5 batches = 1 episode
        )
        
        self.spark = None
        self.start_time = None
    
    def create_spark_session(self):
        """Create Spark session with Kafka support"""
        print_header("CREATING SPARK SESSION")
        
        self.spark = SparkSession.builder \
            .appName("IoT-Streaming-FullRL") \
            .master("local[*]") \
            .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0") \
            .config("spark.executor.instances", str(self.rl_controller.current_executors)) \
            .config("spark.sql.streaming.checkpointLocation", "/tmp/spark-checkpoints-fullrl") \
            .config("spark.sql.shuffle.partitions", "4") \
            .getOrCreate()
        
        self.spark.sparkContext.setLogLevel("WARN")
        
        print_metric("App Name", "IoT-Streaming-FullRL")
        print_metric("Master", "local[*]")
        print_metric("Kafka Bootstrap", self.kafka_bootstrap)
        print_metric("Topic", self.kafka_topic)
        print_metric("Initial Executors", str(self.rl_controller.current_executors))
        print_metric("Phase-1 Model", "PPO (trained)" if self.rl_controller.phase1_model else "Fallback")
        print_metric("Phase-2 Model", "TD3 (trained)" if self.rl_controller.phase2_model else "Fallback")
        
        return self.spark
    
    def process_batch_callback(self, batch_df, batch_id):
        """
        Callback for each micro-batch.
        Uses TRAINED RL models for decisions!
        """
        batch_start = time.time()
        
        # Get batch stats
        try:
            row_count = batch_df.count()
        except:
            row_count = 0
        
        if row_count == 0:
            return
        
        # Collect metrics
        processing_time = (time.time() - batch_start) * 1000
        simulated_latency = 50 + row_count * 2 + np.random.uniform(-20, 20)
        
        batch_metrics = {
            "message_count": row_count,
            "workload": row_count * 10,
            "latency_ms": simulated_latency,
            "cpu_util": builtins.min(100, 30 + row_count * 0.5),
            "memory_util": builtins.min(100, 25 + row_count * 0.4),
            "throughput": row_count / (processing_time / 1000) if processing_time > 0 else row_count,
            "shuffle_mb": row_count * 0.1,
            "data_temp": 0.7,
            "edge_workload": row_count * 12,
            "burst_signal": 1.0 if row_count > 50 else 0.0
        }
        
        # RL Decision (using trained models!)
        rl_result = self.rl_controller.process_batch(batch_metrics)
        
        # Print batch summary
        sla_ok = "âœ“" if simulated_latency <= self.rl_controller.sla_target else "âœ—"
        model_indicator = f"{Colors.MAGENTA}ğŸ§  NN{Colors.ENDC}" if self.rl_controller.phase1_model else "ğŸ“‹"
        
        print(f"\n  {Colors.CYAN}â”Œâ”€ BATCH {batch_id} â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”{Colors.ENDC}")
        print(f"  {Colors.CYAN}â”‚{Colors.ENDC}  Messages: {row_count:5d}  |  Latency: {simulated_latency:6.1f}ms {sla_ok}")
        print(f"  {Colors.CYAN}â”‚{Colors.ENDC}  {model_indicator} Action: {rl_result['action']:10s}  |  Executors: {rl_result['executors']}")
        print(f"  {Colors.CYAN}â”‚{Colors.ENDC}  Reward: {rl_result['reward']:+.4f}  |  Weights: Î±={self.rl_controller.alpha:.2f}, Î²={self.rl_controller.beta:.2f}, Î³={self.rl_controller.gamma:.2f}")
        
        if rl_result["episode_complete"]:
            meta_indicator = f"{Colors.MAGENTA}ğŸ§  TD3{Colors.ENDC}" if self.rl_controller.phase2_model else "ğŸ“‹"
            print(f"  {Colors.CYAN}â”‚{Colors.ENDC}")
            print(f"  {Colors.YELLOW}â”‚  {meta_indicator} EPISODE {rl_result['episode']} COMPLETE!{Colors.ENDC}")
            print(f"  {Colors.YELLOW}â”‚     Meta-Reward: {rl_result['meta_reward']:+.4f}{Colors.ENDC}")
            if "new_weights" in rl_result:
                nw = rl_result["new_weights"]
                print(f"  {Colors.YELLOW}â”‚     New Weights: Î±={nw['alpha']:.3f}, Î²={nw['beta']:.3f}, Î³={nw['gamma']:.3f}{Colors.ENDC}")
        
        print(f"  {Colors.CYAN}â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜{Colors.ENDC}")
    
    def start_streaming(self, timeout_seconds: Optional[int] = None):
        """Start the Spark streaming job with FULL RL integration"""
        
        print_header("SPARK STREAMING WITH FULLY TRAINED RL (NO RULES!)")
        
        # Create session
        self.create_spark_session()
        
        # Define schema for IoT data
        schema = StructType([
            StructField("sensor_id", StringType(), True),
            StructField("sensor_type", StringType(), True),
            StructField("value", DoubleType(), True),
            StructField("timestamp", StringType(), True),
            StructField("location", StringType(), True)
        ])
        
        print(f"\n  {Colors.GREEN}Starting stream from Kafka topic: {self.kafka_topic}{Colors.ENDC}")
        print(f"  {Colors.GREEN}Batch interval: {self.batch_interval}{Colors.ENDC}")
        print(f"  {Colors.GREEN}Episode length: {self.rl_controller.episode_length} batches{Colors.ENDC}")
        print(f"\n  {Colors.MAGENTA}ğŸ§  ALL DECISIONS BY NEURAL NETWORKS - NO HARDCODED RULES!{Colors.ENDC}")
        print(f"\n  {Colors.YELLOW}Waiting for data...{Colors.ENDC}\n")
        
        self.start_time = time.time()
        
        try:
            # Read from Kafka
            df = self.spark \
                .readStream \
                .format("kafka") \
                .option("kafka.bootstrap.servers", self.kafka_bootstrap) \
                .option("subscribe", self.kafka_topic) \
                .option("startingOffsets", "latest") \
                .option("failOnDataLoss", "false") \
                .load()
            
            # Parse JSON
            parsed_df = df.select(
                F.col("key").cast("string").alias("sensor_id"),
                F.from_json(F.col("value").cast("string"), schema).alias("data"),
                F.col("timestamp").alias("kafka_timestamp")
            ).select("sensor_id", "data.*", "kafka_timestamp")
            
            # Write stream with foreachBatch for RL processing
            query = parsed_df \
                .writeStream \
                .foreachBatch(self.process_batch_callback) \
                .outputMode("append") \
                .trigger(processingTime=self.batch_interval) \
                .start()
            
            if timeout_seconds:
                query.awaitTermination(timeout_seconds * 1000)
            else:
                query.awaitTermination()
                
        except KeyboardInterrupt:
            print(f"\n\n  {Colors.YELLOW}Stream stopped by user{Colors.ENDC}")
        except Exception as e:
            print(f"\n  {Colors.RED}Error: {e}{Colors.ENDC}")
        finally:
            self.print_summary()
            if self.spark:
                self.spark.stop()
    
    def print_summary(self):
        """Print final summary"""
        duration = time.time() - self.start_time if self.start_time else 0
        
        print_header("STREAMING SESSION COMPLETE")
        print(f"""
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                     FULL RL SESSION STATISTICS                           â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚  Duration:              {duration:>8.1f} seconds                               â”‚
    â”‚  Total Batches:         {self.rl_controller.total_batches:>8}                                   â”‚
    â”‚  Total Messages:        {self.rl_controller.total_messages:>8}                                   â”‚
    â”‚  Total Episodes:        {self.rl_controller.episode_count:>8}                                   â”‚
    â”‚  SLA Violations:        {self.rl_controller.total_violations:>8}                                   â”‚
    â”‚  Final Executors:       {self.rl_controller.current_executors:>8}                                   â”‚
    â”‚                                                                          â”‚
    â”‚  ğŸ§  Models Used:                                                         â”‚
    â”‚    Phase-1 (Scaling): {"PPO (trained)" if self.rl_controller.phase1_model else "Fallback":>20}                           â”‚
    â”‚    Phase-2 (Weights): {"TD3 (trained)" if self.rl_controller.phase2_model else "Fallback":>20}                           â”‚
    â”‚                                                                          â”‚
    â”‚  Final Weights (learned by TD3):                                         â”‚
    â”‚    Î± (cost):       {self.rl_controller.alpha:>8.3f}                                          â”‚
    â”‚    Î² (latency):    {self.rl_controller.beta:>8.3f}                                          â”‚
    â”‚    Î³ (throughput): {self.rl_controller.gamma:>8.3f}                                          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        """)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MAIN
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="Spark Streaming with FULLY TRAINED RL")
    parser.add_argument("--kafka-bootstrap", default="localhost:9092", help="Kafka bootstrap servers")
    parser.add_argument("--topic", default="iot-realtime-demo", help="Kafka topic to consume")
    parser.add_argument("--batch-interval", default="5 seconds", help="Batch interval")
    parser.add_argument("--timeout", type=int, default=None, help="Timeout in seconds (None = run forever)")
    
    args = parser.parse_args()
    
    print_header("SPARK STREAMING WITH FULLY TRAINED RL MODELS")
    print(f"""
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                                                          â”‚
    â”‚   ğŸ“¨ Kafka â†’ âš¡ Spark â†’ ğŸ§  PPO Neural Network â†’ ğŸ”§ Scaling Action        â”‚
    â”‚                             â†“                                            â”‚
    â”‚                        ğŸ§  TD3 Neural Network â†’ âš–ï¸ New Weights            â”‚
    â”‚                             â†“                                            â”‚
    â”‚                        ğŸ’¾ Redis (state)                                  â”‚
    â”‚                                                                          â”‚
    â”‚   âš ï¸  NO HARDCODED RULES OR THRESHOLDS!                                  â”‚
    â”‚   âœ… All decisions made by trained neural networks                       â”‚
    â”‚                                                                          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)
    
    streaming = SparkWithFullRL(
        kafka_bootstrap=args.kafka_bootstrap,
        kafka_topic=args.topic,
        batch_interval=args.batch_interval
    )
    
    streaming.start_streaming(timeout_seconds=args.timeout)


if __name__ == "__main__":
    main()
