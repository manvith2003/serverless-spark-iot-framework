# üéì Two-RL System Demo Guide
## For Thesis Defense Presentation

---

## üìÅ Project Structure Overview

```
serverless-spark-iot-framework/
‚îú‚îÄ‚îÄ rl_core/                          # üß† CORE RL LOGIC (MOST IMPORTANT!)
‚îÇ   ‚îú‚îÄ‚îÄ continuous_two_rl_loop.py     # ‚≠ê Two-RL system main loop
‚îÇ   ‚îú‚îÄ‚îÄ continuous_scaler.py          # RL ‚Üí Spark integration
‚îÇ   ‚îú‚îÄ‚îÄ phase2_meta_controller.py     # TD3/PPO Meta-Controller
‚îÇ   ‚îî‚îÄ‚îÄ multi_objective_scheduler.py  # Multi-objective RL scheduler
‚îÇ
‚îú‚îÄ‚îÄ optimization/resource_allocation/ # Phase-1 RL Implementation
‚îÇ   ‚îú‚îÄ‚îÄ ppo_agent.py                  # ‚≠ê PPO agent for scaling
‚îÇ   ‚îî‚îÄ‚îÄ rl_environment.py             # Gym environment for Spark
‚îÇ
‚îú‚îÄ‚îÄ spark_core/streaming/             # Spark Integration
‚îÇ   ‚îú‚îÄ‚îÄ spark_streaming_job.py        # Base Spark streaming
‚îÇ   ‚îî‚îÄ‚îÄ spark_with_two_rl.py          # ‚≠ê Spark + Two-RL integration
‚îÇ
‚îú‚îÄ‚îÄ demos/                            # Demo Scripts
‚îÇ   ‚îî‚îÄ‚îÄ end_to_end_pipeline_demo.py   # ‚≠ê Full pipeline demo
‚îÇ
‚îú‚îÄ‚îÄ benchmarks/                       # Benchmarking
‚îÇ   ‚îî‚îÄ‚îÄ rl_vs_dynamic_allocation.py   # RL vs Spark comparison
‚îÇ
‚îî‚îÄ‚îÄ generate_real_graphs.py           # Graph generation script
```

---

## üñ•Ô∏è Terminal Commands to Run the System

### 1. Navigate to Project
```bash
cd /Users/manvithm/serverless-spark-iot-framework/serverless-spark-iot-framework
```

### 2. Demo Option 1: Full End-to-End Pipeline Demo (RECOMMENDED)
```bash
python demos/end_to_end_pipeline_demo.py
```
**What it shows**: Complete pipeline from IoT ‚Üí MQTT ‚Üí Kafka ‚Üí Spark ‚Üí RL decisions

### 3. Demo Option 2: Two-RL Continuous Loop
```bash
python rl_core/continuous_two_rl_loop.py
```
**What it shows**: Phase-1 and Phase-2 RL learning together

### 4. Demo Option 3: Continuous Scaling Loop
```bash
python rl_core/continuous_scaler.py
```
**What it shows**: RL ‚Üí Resource Manager (YARN/K8s) integration

### 5. Demo Option 4: Phase-2 Meta-Controller Only
```bash
python rl_core/phase2_meta_controller.py
```
**What it shows**: TD3/PPO training for (Œ±, Œ≤, Œ≥) weights

### 6. Generate Real Training Graphs
```bash
python generate_real_graphs.py
```
**What it creates**: ppo_reward_curve.png, td3_weights_curve.png, benchmark_comparison_real.png

### 7. Run Benchmark Comparison
```bash
python benchmarks/rl_vs_dynamic_allocation.py
```
**What it shows**: RL vs Spark Dynamic Allocation comparison

---

## üîó Key Code Lines: RL Integration Points

### üìå WHERE RL IS INTEGRATED WITH SPARK
**File**: `rl_core/continuous_scaler.py`

| Line Range | What It Does |
|------------|--------------|
| **Line 174-236** | THE CONTINUOUS LOOP - runs every N seconds |
| **Line 199-205** | RL scheduler makes scaling decision |
| **Line 209-216** | Decision sent to Resource Manager (YARN/K8s) |

**Key Code Snippet** (Lines 199-216):
```python
# STEP 2: GET RL DECISION
decision = self.rl_scheduler.get_scaling_decision(metrics)

logger.info(f"   ü§ñ RL Decision: {decision['action'].upper()} ‚Üí "
           f"{decision['target_executors']} executors")

# STEP 3: APPLY TO RESOURCE MANAGER
if target != current:
    logger.info(f"   üîß Calling Resource Manager: {current} ‚Üí {target} executors")
    result = self.rm_client.request_executors(target)
```

---

### üìå WHERE PHASE-2 (TD3) IS INTEGRATED WITH PHASE-1 (PPO)
**File**: `rl_core/continuous_two_rl_loop.py`

| Line Range | What It Does |
|------------|--------------|
| **Line 199-215** | Phase-1 reward uses weights FROM Phase-2 |
| **Line 259-279** | Phase-2 outputs (Œ±, Œ≤, Œ≥) weights |
| **Line 418-461** | Phase-2 UPDATE after episode - learns and adjusts weights |

**Key Integration Code** (Lines 199-215):
```python
def _compute_phase1_reward(self, metrics: StepMetrics) -> float:
    """
    Phase-1 reward: Uses weights from Phase-2
    
    R = -Œ±¬∑cost - Œ≤¬∑latency + Œ≥¬∑throughput
    """
    reward = (
        -self.alpha * norm_cost +      # Œ± from Phase-2
        -self.beta * norm_latency +    # Œ≤ from Phase-2
        self.gamma * norm_throughput   # Œ≥ from Phase-2
    )
    return reward
```

**Phase-2 Outputs Weights** (Lines 259-279):
```python
def _phase2_get_weights(self, state: np.ndarray) -> Tuple[float, float, float]:
    """Get (Œ±, Œ≤, Œ≥) from Phase-2 (or heuristic if no model)"""
    if self.phase2_model is not None:
        action, _ = self.phase2_model.predict(state, deterministic=False)
        weights = exp_action / exp_action.sum()
        return (weights[0], weights[1], weights[2])  # Œ±, Œ≤, Œ≥
```

**Phase-2 Update After Episode** (Lines 418-461):
```python
def update_phase2(self, episode_metrics: EpisodeMetrics):
    """
    Phase-2 learning after episode completes.
    This is where Phase-2 learns and updates (Œ±, Œ≤, Œ≥).
    """
    # Get new weights from Phase-2
    new_weights = self._phase2_get_weights(state)
    
    # Phase-2 learning
    self._phase2_learn()
    
    # Update weights for next episode
    self.alpha, self.beta, self.gamma = new_weights
```

---

## üéØ Quick Reference: Files to Show During Demo

| Purpose | File | Key Lines |
|---------|------|-----------|
| **Two-RL Main Loop** | `rl_core/continuous_two_rl_loop.py` | 322-416 (run_one_episode) |
| **RL ‚Üí Spark Integration** | `rl_core/continuous_scaler.py` | 174-236 (_scaling_loop) |
| **Phase-1 PPO Agent** | `optimization/resource_allocation/ppo_agent.py` | 101-139 (predict) |
| **Phase-2 Meta-Controller** | `rl_core/phase2_meta_controller.py` | 282-377 (Phase2PPOController) |
| **Full Pipeline Demo** | `demos/end_to_end_pipeline_demo.py` | 234-437 (run_demo) |

---

## üé¨ Recommended Demo Flow

### Step 1: Show Architecture (30 sec)
- Show the generated architecture diagram
- Explain: "IoT sensors ‚Üí MQTT ‚Üí Kafka ‚Üí Spark ‚Üê Two-RL System"

### Step 2: Run End-to-End Demo (2 min)
```bash
python demos/end_to_end_pipeline_demo.py
```
- Point out Phase-1 decisions (scaling)
- Point out Phase-2 decisions (weight adaptation)

### Step 3: Show Key Code (1 min)
Open `rl_core/continuous_two_rl_loop.py`:
- **Line 209-213**: Show how Phase-1 reward uses Œ±, Œ≤, Œ≥ from Phase-2
- **Line 447-448**: Show how Phase-2 updates weights between episodes

### Step 4: Show Real Training Graphs (30 sec)
```bash
python generate_real_graphs.py
```
- Open the generated PNG files
- Explain convergence patterns

### Step 5: Show Benchmark Comparison (1 min)
```bash
python benchmarks/rl_vs_dynamic_allocation.py
```
- Show RL outperforms Spark Dynamic Allocation
- Highlight proactive vs reactive scaling

---

## üí° Key Points to Emphasize

1. **Two-Phase Design**:
   - Phase-1 (PPO): Fast, tactical, per-batch scaling
   - Phase-2 (TD3): Slow, strategic, per-episode weight tuning

2. **Integration Points**:
   - Phase-2 ‚Üí Phase-1: Via (Œ±, Œ≤, Œ≥) weights
   - Phase-1 ‚Üí Spark: Via Resource Manager API

3. **Why Two-RL?**:
   - Single RL can't handle both tactical (ms) and strategic (hours) decisions
   - Separation of concerns: one brain for speed, one for strategy

4. **Real Results**:
   - 27% cost reduction
   - 100% SLA compliance
   - Proactive scaling (anticipates bursts)

---

## üîß Troubleshooting

If any command fails:
```bash
# Ensure you're in the right directory
cd /Users/manvithm/serverless-spark-iot-framework/serverless-spark-iot-framework

# Check Python version
python3 --version

# Install missing dependencies if needed
pip install numpy matplotlib
```

---

**Good luck with your presentation! üéì**
