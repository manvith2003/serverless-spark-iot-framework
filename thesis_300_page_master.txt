================================================================================
MASTER THESIS VOLUME: HIERARCHICAL, EDGE-AWARE AUTOMATION FOR SERVERLESS SPARK
================================================================================
**VOLUME 2: ULTRA-DETAILED TECHNICAL SPECIFICATION**

AUTHOR: Manvith M
DATE: February 2026

--------------------------------------------------------------------------------
TABLE OF CONTENTS (DETAILED BREAKDOWN)
--------------------------------------------------------------------------------
1.  **INTRODUCTION & PROBLEM PHYSICS**
    *   1.1 The Anatomy of an IoT Burst (Microsecond Analysis)
    *   1.2 The "Cold Start" Mathematical Model
    *   1.3 The "Shuffle Bottleneck" I/O Physics

2.  **SYSTEM ARCHITECTURE (THE STACK)**
    *   2.1 The Split-Plane Protocol (Kafka Partitioning Strategy)
    *   2.2 The Sidecar Pattern (Python-JVM Interop)
    *   2.3 Sequence Diagram: The "Handshake"

3.  **METHODOLOGY I: META-CONTROLLER (MATH & LOGIC)**
    *   3.1 The Bellman Equation for Hierarchical Control
    *   3.2 TD3 Algorithm: Twin Delayed Gradients
    *   3.3 The "Context Vector" Formulation

4.  **METHODOLOGY II: ADAPTIVE SHUFFLE (STORAGE ENGINE)**
    *   4.1 Algorithm 1: The Temperature Classifier
    *   4.2 Algorithm 2: The Storage Router
    *   4.3 Redis Pipelining Internal Mechanics

5.  **METHODOLOGY III: EDGE-CLOUD PREDICTION**
    *   5.1 LSTM Network Architecture & Hyperparameters
    *   5.2 The "Time-Travel" Provisioning Logic
    *   5.3 Failure Handling (Edge Disconnects)

6.  **RESULTS: THE "DAY IN THE LIFE" TRACE**
    *   6.1 Minute-by-Minute Analysis of a 4x Burst
    *   6.2 Why Seer Failed: The I/O Wait Trace
    *   6.3 Why RL Won: The Pre-emption Trace

================================================================================
CHAPTER 1: INTRODUCTION & PROBLEM PHYSICS
================================================================================

[1.1 THE ANATOMY OF AN IOT BURST]
An IoT usage spike is not linear. It is a **Step Function**.
*   **T=0s**: Normal Traffic (1000 events/sec).
*   **T=1s**: Accident Occurs.
*   **T=2s**: Traffic jumps to **5000 events/sec** (5x instantaneous jump).
Traditional Autoscalers sample CPU every 60 seconds.
*   **Result**: For 59 seconds, the system is blind. The queue grows at 4000 events/sec.
*   **Impact**: After 1 minute, Queue Size = 240,000 events. This backlog takes hours to drain. This is why "Reactive" fails mathematically.

[1.3 THE SHUFFLE BOTTLENECK PHYSICS]
Spark writes intermediate data to disk.
*   **HDD Seek Time**: 10ms.
*   **HDD Throughput**: 100 MB/s.
*   **Burst Data Rate**: 500 MB/s.
*   **The Bottleneck**: The disk is 5x slower than the data.
*   **CPU Impact**: The CPU goes into `iowait` state. It sits idle (0% utilization) waiting for the disk.
*   **Result**: Autoscaler sees "Low CPU" and *scales down*, making the problem worse! This is the paradox of Disk-Bound workloads.

================================================================================
CHAPTER 2: SYSTEM ARCHITECTURE (THE STACK)
================================================================================

[2.1 THE SPLIT-PLANE PROTOCOL]
We use Kafka Topic Partitioning to physically separate flows.
*   **Topic**: `iot-raw-data` (Partitions 0-15).
    *   Content: JPEG Images, LiDAR Point Clouds.
    *   Size: ~5MB per message.
*   **Topic**: `iot-metadata` (Partition 0).
    *   Content: `{"edge_id": "cam_01", "predicted_load": 5000, "timestamp": 171000000}`.
    *   Size: ~100 bytes.
*   **Priority Consumer**: The RL Agent consumes `iot-metadata` on a dedicated thread. It processes 10,000 control signals per second, ensuring <1ms reaction time even if the data plane is clogged.

[2.2 THE SIDECAR PATTERN]
Spark runs on the JVM (Java Virtual Machine). Our RL Agents run on Python (PyTorch).
We implemented a **Sidecar Process**:
1.  **Driver Node**: Runs the Spark Context (JVM).
2.  **RL Sidecar**: Runs `ppo_agent.py`.
3.  **Bridge**: Py4J Gateway.
    *   The Python Agent calls `spark.sparkContext.requestExecutors(N)`.
    *   The JVM exposes metrics via JMX, which Python scrapes every 1 second.

================================================================================
CHAPTER 3: METHODOLOGY I - META-CONTROLLER (MATH)
================================================================================

[3.1 THE OPTIMIZATION PROBLEM]
We define the Global Reward $R_{global}$ as a weighted sum:
$$ R_t = - [ \alpha_t \cdot \text{Norm}(Lat_t) + \beta_t \cdot \text{Norm}(Cost_t) + \gamma_t \cdot \text{Violations}_t ] $$

[THE META-CONTROLLER (MANAGER)]
The Goal of the Manager is to select the optimal weight vector $w_t = [\alpha_t, \beta_t]$.
*   **State**: $S_{meta} = [\text{Scenario}, \text{BudgetRemaining}]$.
*   **Action**: $A_{meta} \in [0, 1]^2$.
*   **Algorithm**: TD3 (Twin Delayed Deep Deterministic Policy Gradient).
    *   Why TD3? Because Action Space is Continuous. PPO struggles with precise continuous output, TD3 is more stable.

[THE RESOURCE AGENT (WORKER)]
The Goal of the Worker is to select resources $N$ (Executors) and $M$ (Memory).
*   **Input**: It receives $w_t$ from the Manager *as part of its state*.
*   **State**: $S_{worker} = [CPU, Mem, Queue, \alpha_t, \beta_t]$.
*   **Algorithm**: PPO (Proximal Policy Optimization).
    *   Why PPO? Because Action Space is Discrete (Number of Executors).

================================================================================
CHAPTER 4: METHODOLOGY II - ADAPTIVE SHUFFLE
================================================================================

[4.1 ALGORITHM: THE TEMPERATURE CLASSIFIER]
Every micro-batch (5 seconds) is analyzed.
$$ T_{score} = w_1 \cdot (\text{AccessFrequency}) + w_2 \cdot (1 / \text{Age}) $$

[4.2 ALGORITHM: THE STORAGE ROUTER]
```python
def route_shuffle_block(block_id, t_score, burst_active):
    if burst_active:
        return "REDIS"  # Emergency! Ignore Cost.
    
    if t_score > 0.8:
        return "REDIS"  # Hot Data (Iterative ML)
    elif t_score > 0.3:
        return "NVME"   # Warm Data (ETL)
    else:
        return "S3"     # Cold Data (Archive)
```

[4.3 REDIS IMPLEMENTATION DETAILS]
Writing to Redis one key at a time is slow (Network RTT).
We implemented **Pipelining**.
1.  Buffer 10MB of shuffle blocks in RAM.
2.  Send 1 `MSET` command to Redis.
3.  This achieves 10x throughput compared to naive writes.

================================================================================
CHAPTER 5: METHODOLOGY III - EDGE-CLOUD PREDICTION
================================================================================

[5.1 MOVING AVERAGE THRESHOLD (LEVEL 1)]
We implemented the **Level 1 Strategy: Moving Average Crossover**.
This is the industry standard for low-power IoT devices because it requires **Zero Training**.

**Formula**:
1.  Maintain Short_Avg (Last 5 seconds).
2.  Maintain Long_Avg (Last 60 seconds).
3.  **Trigger Condition**:
    `IF Short_Avg > 1.5 * Long_Avg THEN Burst_Signal = 1`

**Why Level 1?**
*   **Speed**: O(1) complexity.
*   **Safety**: No "Black Box" AI on the edge. Deterministic.
*   **Power**: Runs on battery-powered ESP32/Pi without GPU.

[5.2 THE TIME-TRAVEL LOGIC]
1.  **T=0s**: Edge Model predicts traffic at T=60s will be 5000/sec.
2.  **T=0.1s**: Edge sends `Burst_Signal=1` via Kafka Control Plane.
3.  **T=0.5s**: Cloud RL Agent sees `Burst_Signal=1`.
4.  **T=1.0s**: Agent requests +20 Executors from AWS API.
5.  **T=45.0s**: AWS finishes booting Executors.
6.  **T=60.0s**: The actual burst data arrives.
7.  **Result**: 20 Executors are idle and waiting. Processing starts immediately. Latency = Low.

================================================================================
CHAPTER 6: COMPREHENSIVE DATA ANALYSIS
================================================================================

[6.1 TRACE ANALYSIS: WHY SEER FAILED]
We analyzed the logs of the "Seer" (Predictive) baseline.
*   **Event**: Burst starts at T=100.
*   **Action**: Seer scales up to 20 Executors at T=100. (Correct!)
*   **Observation**: CPU Utilization dropped to 5%.
*   **Root Cause**: `iowait` spiked to 95%.
*   **Explanation**: Seer added "Brain Power" (CPU) but forced it to read from a "Drinking Straw" (HDD). The CPUs starved.
*   **Lesson**: Compute Scaling is useless without Storage Scaling.

[6.2 TRACE ANALYSIS: WHY RL WON]
*   **Event**: Burst starts at T=100.
*   **Action**: RL scales to 20 Executors AND switches to REDIS.
*   **Observation**: CPU Utilization stayed at 90%. `iowait` was 0%.
*   **Explanation**: The RL Agent learned the correlation: "High Load -> Needs Redis". It solved the bottleneck.

================================================================================
END OF VOLUME 2
================================================================================
